# Benchmarking MILS
 ### The point of this excercise is to compare MILS to other benchmark applications (GPT-4V and BLIP-2)

#### We will conduct 3 Experiments:

1. **Image Captioning** \
**Prompt:** "Describe this image in detail." \
**Evaluation Metrics:** BLEU, METEOR, CIDEr, SPICE \
**Human Evaluation:** Fluency + relevance (1â€“5 scale)

    **Dataset:** 1K images from COCO Val2014

2. **Visual Question Answering (VQA)** \
**Prompt:** Natural language questions (e.g., â€œWhat is the boy doing?â€) \
**Evaluation Metrics:** Exact match with ground truth answers \
**Additional:** Optional visual reasoning questions

    **Dataset:** VQAv2

3. **Referring Expressions** \
**Prompt:** â€œPoint to the red carâ€ or â€œHighlight the person wearing a hat.â€ \
**Output:** Either a bounding box or textual description of the region. \
**Evaluation Metrics:** Intersection over Union (IoU), referring accuracy

    **Dataset:** RefCOCO

##### Tools:

ğŸ§  MILS (access from Meta repo or API) \
ğŸ¤– BLIP-2: Salesforce GitHub \
ğŸ“· GPT-4V: Use OpenAIâ€™s gpt-4-vision-preview model (if available) \
ğŸ“¦ Hugging Face for dataset loading: datasets, torchvision \
ğŸ“Š Evaluation libraries: nlg-eval, pycocoevalcap, scikit-image
